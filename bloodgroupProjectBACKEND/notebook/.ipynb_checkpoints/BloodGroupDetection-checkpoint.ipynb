{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = r\"D:/project/bloodgroupdetectionprject/dataset\"\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    labels=\"inferred\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Step 2: Check class distribution\n",
    "class_names = dataset.class_names\n",
    "class_counts = Counter()\n",
    "\n",
    "for _, labels in dataset.unbatch():\n",
    "    class_counts[int(labels.numpy())] += 1\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "for i, count in class_counts.items():\n",
    "    print(f\"{class_names[i]}: {count}\")\n",
    "\n",
    "def plot_class_distribution(class_names, class_counts):\n",
    "    classes = [class_names[i] for i in class_counts.keys()]\n",
    "    counts = [class_counts[i] for i in class_counts.keys()]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(classes, counts)\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(class_names, class_counts)\n",
    "\n",
    "# Step 3: Balance the dataset\n",
    "max_count = max(class_counts.values())\n",
    "\n",
    "def oversample_class(class_id, count, max_count):\n",
    "    unbatched_dataset = dataset.unbatch()\n",
    "    class_dataset = unbatched_dataset.filter(lambda img, lbl: tf.equal(lbl, class_id))\n",
    "    repeat_factor = max_count // count + (max_count % count > 0)\n",
    "    return class_dataset.repeat(repeat_factor).take(max_count)\n",
    "\n",
    "balanced_datasets = []\n",
    "for class_id, count in class_counts.items():\n",
    "    balanced_datasets.append(oversample_class(class_id, count, max_count))\n",
    "\n",
    "balanced_dataset = tf.data.Dataset.sample_from_datasets(balanced_datasets)\n",
    "balanced_dataset = balanced_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# Re-check class balance\n",
    "balanced_class_counts = Counter()\n",
    "for _, label in balanced_dataset.unbatch():\n",
    "    balanced_class_counts[int(label.numpy())] += 1\n",
    "\n",
    "plot_class_distribution(class_names, balanced_class_counts)\n",
    "\n",
    "# Step 4: Split the dataset\n",
    "train_ratio, val_ratio = 0.7, 0.2\n",
    "\n",
    "balanced_dataset_unbatched = balanced_dataset.unbatch()\n",
    "dataset_size = sum(1 for _ in balanced_dataset_unbatched)\n",
    "\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "\n",
    "balanced_dataset_unbatched = balanced_dataset.unbatch()\n",
    "train_dataset = balanced_dataset_unbatched.take(train_size)\n",
    "val_test_dataset = balanced_dataset_unbatched.skip(train_size)\n",
    "val_dataset = val_test_dataset.take(val_size)\n",
    "test_dataset = val_test_dataset.skip(val_size)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(f\"Training dataset size: {train_size}\")\n",
    "print(f\"Validation dataset size: {val_size}\")\n",
    "print(f\"Testing dataset size: {dataset_size - train_size - val_size}\")\n",
    "\n",
    "# Step 5: Define the model\n",
    "def create_high_accuracy_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "        tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "high_acc_model = create_high_accuracy_model()\n",
    "\n",
    "# Step 6: Callbacks\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Step 7: Training\n",
    "history_high_acc = high_acc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "# Step 8: Evaluation\n",
    "high_acc_eval = high_acc_model.evaluate(test_dataset)\n",
    "print(f\"High Accuracy Model - Loss: {high_acc_eval[0]}, Accuracy: {high_acc_eval[1]}\")\n",
    "\n",
    "# Step 9: Accuracy Plot\n",
    "def plot_accuracy(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy(history_high_acc)\n",
    "\n",
    "# Step 10: Confusion Matrix\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    predictions = high_acc_model.predict(images)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(predicted_labels)\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Save the model\n",
    "high_acc_model.save('model.h5')\n",
    "print(\"Model saved as model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
